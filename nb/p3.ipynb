{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb55aaf7-0e70-43f1-9b80-d777266ea0e3",
   "metadata": {},
   "source": [
    "# contributors:\n",
    "simran4@wisc.edu, rgundavarapu@wisc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159e3f8-7200-4495-a1e8-e476cb1ed5de",
   "metadata": {},
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1327d42-d56d-4ca2-b8aa-33eaf2ec3e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  166M  100  166M    0     0  12.8M      0  0:00:13  0:00:13 --:--:-- 10.4M\n"
     ]
    }
   ],
   "source": [
    "!curl https://pages.cs.wisc.edu/~harter/cs639/data/hdma-wi-2021.csv > wi-2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab3431f-3a60-4169-a290-b125323d9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -D dfs.block.size=1048576 -D dfs.replication=1 -cp wi-2021.csv hdfs://main:9000/single.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa23a22-9c68-4156-9ff6-a4359faee85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -D dfs.block.size=1048576 -D dfs.replication=2 -cp wi-2021.csv hdfs://main:9000/double.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4675078-b2d2-4e3f-8baf-d160e590c99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166.8 M  333.7 M  hdfs://main:9000/double.csv\n",
      "166.8 M  166.8 M  hdfs://main:9000/single.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du -h hdfs://main:9000/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3143d-24f6-4dd9-b7d0-84688eb6a351",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f05c5a-17e5-4a38-9460-e60ff50d04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ea1b9c-1f14-405b-9c90-79fc7704a76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http://a7cc8c8e1c27:9864/webhdfs/v1/single.csv': 92,\n",
       " 'http://87931e77d04b:9864/webhdfs/v1/single.csv': 75}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks = {}\n",
    "for i in range(167):\n",
    "    url = \"http://main:9870/webhdfs/v1/single.csv?op=OPEN&offset=\"+str(i*1024*1024)\n",
    "    resp = requests.get(url, allow_redirects=False)\n",
    "    keyval = resp.headers[\"Location\"].split('?')[0]\n",
    "    if keyval in blocks.keys():\n",
    "        blocks[keyval] = blocks[keyval] + 1\n",
    "    else:\n",
    "        blocks[keyval] = 1\n",
    "blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e59309-9109-4b92-b607-7fe546ce7d27",
   "metadata": {},
   "source": [
    "# PART 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b6f002f-3106-4f4f-9044-72c06b9f5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "class hdfsFile(io.RawIOBase):\n",
    "    def __init__(self, path):\n",
    "        self.path = f\"http://main:9870/webhdfs/v1/{path}\"\n",
    "        self.offset = 0\n",
    "        resp=requests.get(\"http://main:9870/webhdfs/v1/single.csv?op=GETFILESTATUS\")\n",
    "        resp=(resp.content).decode()\n",
    "        resp=json.loads(resp) # extracting the length by converting byte to string to dict\n",
    "        self.length = resp['FileStatus']['length']\n",
    "        \n",
    "    def readable(self):\n",
    "        return True\n",
    "    \n",
    "    def readinto(self, b):\n",
    "        resp = requests.get(self.path, params = {\"op\": \"OPEN\", \"offset\": self.offset, \"length\": len(b)})\n",
    "        if self.offset > self.length:\n",
    "            return 0\n",
    "        if resp.status_code != 200:\n",
    "            b[:1] = b\"\\n\"\n",
    "            self.offset += 1048576 - (self.offset % 1048576)\n",
    "            return 1\n",
    "        b[:len(resp.content)]=resp.content\n",
    "        self.offset += len(resp.content)\n",
    "        return len(resp.content) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e61465-9d46-4b7b-ae49-e7fced606b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts from single.csv with buffer size 512KB\n",
      "Single Family:  444874\n",
      "Multi Family:  2493\n",
      "Seconds:  21.319695234298706\n"
     ]
    }
   ],
   "source": [
    "single=0\n",
    "multi=0\n",
    "counter = 0\n",
    "import time as time\n",
    "t1=time.time()\n",
    "for line in io.BufferedReader(hdfsFile(\"single.csv\"),buffer_size=524288):\n",
    "    line = str(line, \"utf-8\")\n",
    "    counter+=1\n",
    "    if counter>1:\n",
    "        if 'Single Family' in line:\n",
    "            single+=1\n",
    "        elif 'Multifamily' in line:\n",
    "            multi+=1\n",
    "t2=time.time()\n",
    "print('Counts from single.csv with buffer size 512KB')\n",
    "print('Single Family: ',single)\n",
    "print('Multi Family: ',multi)\n",
    "print('Seconds: ',t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a0568f3-7110-4ec4-b14b-1ee75de8f830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts from double.csv with buffer size 1MB\n",
      "Single Family:  444874\n",
      "Multi Family:  2493\n",
      "Seconds:  9.27297830581665\n"
     ]
    }
   ],
   "source": [
    "double=0\n",
    "multi=0\n",
    "counter = 0\n",
    "import time as time\n",
    "t1=time.time()\n",
    "for line in io.BufferedReader(hdfsFile(\"double.csv\"),buffer_size=1048576):\n",
    "    line = str(line, \"utf-8\")\n",
    "    counter+=1\n",
    "    if counter>1:\n",
    "        if 'Single Family' in line:\n",
    "            double+=1\n",
    "        elif 'Multifamily' in line:\n",
    "            multi+=1\n",
    "t2=time.time()\n",
    "print('Counts from double.csv with buffer size 1MB')\n",
    "print('Single Family: ',double)\n",
    "print('Multi Family: ',multi)\n",
    "print('Seconds: ',t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a3cfa-b8f4-425a-98fb-068531139f86",
   "metadata": {},
   "source": [
    "# PART 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66a5a7a2-b62f-41de-9338-3fa1106a54b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "Present Capacity: 12281319649 (11.44 GB)\n",
      "DFS Remaining: 12025724928 (11.20 GB)\n",
      "DFS Used: 255594721 (243.75 MB)\n",
      "DFS Used%: 2.08%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 167\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 92\n",
      "\tMissing blocks (with replication factor 1): 92\n",
      "\tLow redundancy blocks with highest priority to recover: 167\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 172.18.0.2:9866 (project-3-procrastinators-worker-1.cs544net)\n",
      "Hostname: 87931e77d04b\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 255594721 (243.75 MB)\n",
      "Non DFS Used: 13522956063 (12.59 GB)\n",
      "DFS Remaining: 12025724928 (11.20 GB)\n",
      "DFS Used%: 0.99%\n",
      "DFS Remaining%: 46.57%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Fri Mar 24 03:31:32 GMT 2023\n",
      "Last Block Report: Fri Mar 24 03:21:44 GMT 2023\n",
      "Num of Blocks: 242\n",
      "\n",
      "\n",
      "Dead datanodes (1):\n",
      "\n",
      "Name: 172.18.0.4:9866 (172.18.0.4)\n",
      "Hostname: a7cc8c8e1c27\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 25821052928 (24.05 GB)\n",
      "DFS Used: 273390491 (260.73 MB)\n",
      "Non DFS Used: 13505127525 (12.58 GB)\n",
      "DFS Remaining: 12025757696 (11.20 GB)\n",
      "DFS Used%: 1.06%\n",
      "DFS Remaining%: 46.57%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Fri Mar 24 03:28:35 GMT 2023\n",
      "Last Block Report: Fri Mar 24 03:21:44 GMT 2023\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfsadmin -fs hdfs://main:9000/ -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7ad9dc-122d-4bee-b169-faf3b9d16f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts from single.csv with buffer size 1MB\n",
      "Single Family:  199717\n",
      "Multi Family:  1272\n",
      "Seconds:  3.2457287311553955\n"
     ]
    }
   ],
   "source": [
    "single=0\n",
    "multi=0\n",
    "counter = 0\n",
    "import time as time\n",
    "t1=time.time()\n",
    "for line in io.BufferedReader(hdfsFile(\"single.csv\"),buffer_size=1048576):\n",
    "    line = str(line, \"utf-8\")\n",
    "    counter+=1\n",
    "    if counter>1:\n",
    "        if 'Single Family' in line:\n",
    "            single+=1\n",
    "        elif 'Multifamily' in line:\n",
    "            multi+=1\n",
    "t2=time.time()\n",
    "print('Counts from single.csv with buffer size 1MB')\n",
    "print('Single Family: ',single)\n",
    "print('Multi Family: ',multi)\n",
    "print('Seconds: ',t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87391f9a-7197-4810-82db-8c63df7f8d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts from double.csv with buffer size 1MB\n",
      "Single Family:  444874\n",
      "Multi Family:  2493\n",
      "Seconds:  4.412144899368286\n"
     ]
    }
   ],
   "source": [
    "double=0\n",
    "multi=0\n",
    "counter = 0\n",
    "import time as time\n",
    "t1=time.time()\n",
    "for line in io.BufferedReader(hdfsFile(\"double.csv\"),buffer_size=1048576):\n",
    "    line = str(line, \"utf-8\")\n",
    "    counter+=1\n",
    "    if counter>1:\n",
    "        if 'Single Family' in line:\n",
    "            double+=1\n",
    "        elif 'Multifamily' in line:\n",
    "            multi+=1\n",
    "t2=time.time()\n",
    "print('Counts from double.csv with buffer size 1MB')\n",
    "print('Single Family: ',double)\n",
    "print('Multi Family: ',multi)\n",
    "print('Seconds: ',t2-t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
